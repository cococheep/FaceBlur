{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG16.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOEo8mrWHP4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "!pip install -q tensorflow-gpu==2.0.0-rc1\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QohGIG1ZIZwc",
        "colab_type": "code",
        "outputId": "7a866cc6-8e82-4586-d846-713832e7c770",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaKGF3hhwMmR",
        "colab_type": "text"
      },
      "source": [
        "Dataset Preprocessing\n",
        "\n",
        "Get face label image from 'Wider Face Dataset'\n",
        "\n",
        "Get other label image from VOC2007 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hF_Mv2tQCtM",
        "colab_type": "code",
        "outputId": "e3cf53a0-9413-40e8-f2a8-30dca13024e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# using wider face dataset\n",
        "# http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html\n",
        "import cv2\n",
        "\n",
        "annotation_path = '/content/drive/My Drive/Colab Notebooks/wider_face_train_bbx_gt.txt'\n",
        "annotation = []\n",
        "with open(annotation_path) as f:\n",
        "    for line in f:\n",
        "        annotation.append(line.rstrip())\n",
        "\n",
        "def crop_face(index, bbox):\n",
        "  image_path = '/content/drive/My Drive/Colab Notebooks/images/' + annotation[index] \n",
        "  img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "  crop = img[bbox[1]:bbox[3], bbox[0]:bbox[2], :]\n",
        "  crop = cv2.resize(crop, dsize=(112, 112), interpolation=cv2.INTER_LINEAR)\n",
        "  return crop\n",
        "\n",
        "num_image = 0\n",
        "i = 0\n",
        "max_num = 10000\n",
        "\n",
        "face_image = np.zeros((max_num, 112, 112, 3), dtype=np.uint8)\n",
        "while True:\n",
        "  image_path = annotation[i]\n",
        "  num_face = int(annotation[i+1])\n",
        "  for j in range(num_face):\n",
        "    bbox = annotation[i + 2 + j].split()\n",
        "    [sx, sy, ex, ey] = [0, 0, 0, 0]\n",
        "    \n",
        "    # heavy blur\n",
        "    if int(bbox[4]) == 2 or int(bbox[4]) == 1:\n",
        "      [sx, sy, ex, ey] = [-1, -1, -1, -1]\n",
        "    # extreme illumination\n",
        "    elif int(bbox[6]) == 1:\n",
        "      [sx, sy, ex, ey] = [-1, -1, -1, -1]\n",
        "    # invalid image \n",
        "    elif int(bbox[6]) == 1:\n",
        "      [sx, sy, ex, ey] = [-1, -1, -1, -1]\n",
        "    # hard occlusion\n",
        "    elif int(bbox[8]) == 2:\n",
        "      [sx, sy, ex, ey] = [-1, -1, -1, -1]\n",
        "    # atypical pose\n",
        "    elif int(bbox[9]) == 1:\n",
        "      [sx, sy, ex, ey] = [-1, -1, -1, -1]\n",
        "    # appropriate face    \n",
        "    else:\n",
        "      sx = int(bbox[0])\n",
        "      sy = int(bbox[1])\n",
        "      ex = sx + int(bbox[2])\n",
        "      ey = sy + int(bbox[3])\n",
        "      if int(bbox[3]) > 60:\n",
        "        face_image[num_image] = crop_face(i, [sx, sy, ex, ey])\n",
        "        if num_image == max_num - 1:\n",
        "          break\n",
        "        else:\n",
        "          num_image = num_image + 1\n",
        "  if num_image == max_num - 1:\n",
        "    break\n",
        "  if num_face == 0:\n",
        "    j = 0\n",
        "  i = (i + 2 + j) + 1\n",
        "\n",
        "print(\"[Extracted \" + str(len(face_image))+\" face images from wider face dataset]\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Extracted 10000 face images from wider face dataset]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4Q1B07rjhel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('/content/drive/My Drive/Colab Notebooks/wide_face_image_save', face_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGdiDVljAcCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "face_image = np.load('/content/drive/My Drive/Colab Notebooks/wide_face_image_save.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de32FeHmbYva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load Voc2007 dataset\n",
        "import tensorflow_datasets as tfds\n",
        "voc_builder = tfds.builder('voc')\n",
        "voc_builder.download_and_prepare()\n",
        "voc_ds = voc_builder.as_dataset(split='test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAjtNNoHlxp7",
        "colab_type": "code",
        "outputId": "b2670452-b6f9-4c85-901a-64e7ea30f353",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# get object image from voc2007\n",
        "not_face_image = []\n",
        "i = 0\n",
        "for example in tfds.as_numpy(voc_ds):\n",
        "  image, objects = example['image'], example['objects']\n",
        "  num_objects = len(objects['label'])\n",
        "  for j in range(num_objects):\n",
        "    # find objects not labeled as face (label 'face'=14)\n",
        "    if not objects['label'][j] == 14:\n",
        "      sx = int(objects['bbox'][j][1] * image.shape[1])\n",
        "      sy = int(objects['bbox'][j][0] * image.shape[0])\n",
        "      ex = int(objects['bbox'][j][3] * image.shape[1])\n",
        "      ey = int(objects['bbox'][j][2] * image.shape[0])\n",
        "      crop = image[sy:ey, sx:ex, :]\n",
        "      crop = cv2.resize(crop, dsize=(112, 112), interpolation=cv2.INTER_LINEAR)\n",
        "      crop =  cv2.cvtColor(crop, cv2.COLOR_RGB2BGR)\n",
        "      not_face_image.append(crop)\n",
        "      i = i + 1\n",
        "  \n",
        "print(\"[Extracted \" + str(len(not_face_image)) + \" object images from VOC2007 Dataset]\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Extracted 9749 object images from VOC2007 Dataset]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mjX2zlqArfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert list to numpy array\n",
        "not_face_image = np.asarray(not_face_image)\n",
        "np.save('/content/drive/My Drive/Colab Notebooks/not_face_image_save', not_face_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBYbpHUaAzMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "not_face_image = np.load('/content/drive/My Drive/Colab Notebooks/not_face_image_save.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_N9JnWMo1Tu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# final training data\n",
        "\n",
        "# make label (1: face, 0: other)\n",
        "face_label = np.ones((face_image.shape[0], 1), dtype=int)\n",
        "not_face_label = np.zeros((not_face_image.shape[0], 1), dtype=int)\n",
        "\n",
        "# concatenate two data\n",
        "final_train_image = np.vstack((face_image, not_face_image))\n",
        "final_train_label = np.vstack((face_label, not_face_label))\n",
        "\n",
        "# shuffle dataset\n",
        "dataset = [[x, y] for x, y in zip(final_train_image, final_train_label)]\n",
        "random.shuffle(dataset)\n",
        "\n",
        "x_train = [n[0] for n in dataset]\n",
        "y_train = [n[1] for n in dataset]\n",
        "\n",
        "x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
        "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "\n",
        "x_train = x_train / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scSe5kIzwVh1",
        "colab_type": "text"
      },
      "source": [
        "Training Model:\n",
        "VGG16 with Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3gak6s5HbJP",
        "colab_type": "code",
        "outputId": "4d2c7ea0-660a-4fe3-b4b4-b0b0259b3b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# VGG16 architecture\n",
        "model = models.Sequential()\n",
        "\n",
        "# conv1 (112x112 -> 112x112)\n",
        "model.add(layers.BatchNormalization(input_shape=(112, 112, 3)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer='l2', name='conv1_1'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer='l2', name='conv1_2'))\n",
        "\n",
        "# conv2 (112x112 -> 56x56)\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer='l2', name='conv2_1'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer='l2', name='conv2_2'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool2'))\n",
        "\n",
        "# conv3 (56x56 -> 28x28)\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer='l2', name='conv3_1'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer='l2', name='conv3_2'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer='l2', name='conv3_3'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool3'))\n",
        "\n",
        "# conv4 (28x28 -> 14x14)\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer='l2', name='conv4_1'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer='l2', name='conv4_2'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer='l2', name='conv4_3'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool4'))\n",
        "\n",
        "# conv5 (14x14 -> 7x7)\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer='l2', name='conv5_1'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer='l2', name='conv5_2'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal', kernel_regularizer='l2', name='conv5_3'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool5'))\n",
        "\n",
        "# fc1 (512x7x7 -> 4096)\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(4096, (7, 7), activation='relu', kernel_initializer='he_normal', kernel_regularizer='l2', name='fc1'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "# fc2 (4096 -> 4096)\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dense(4096, activation='relu', name='fc2'))\n",
        "model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "#fc3 (4096 -> 2)\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dense(2, activation='softmax', name='fc3'))\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization_16 (Batc (None, 112, 112, 3)       12        \n",
            "_________________________________________________________________\n",
            "conv1_1 (Conv2D)             (None, 112, 112, 64)      1792      \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 112, 112, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv1_2 (Conv2D)             (None, 112, 112, 64)      36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 112, 112, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv2_1 (Conv2D)             (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 112, 112, 128)     512       \n",
            "_________________________________________________________________\n",
            "conv2_2 (Conv2D)             (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "pool2 (MaxPooling2D)         (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv3_1 (Conv2D)             (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 56, 56, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv3_2 (Conv2D)             (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 56, 56, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv3_3 (Conv2D)             (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "pool3 (MaxPooling2D)         (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv4_1 (Conv2D)             (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 28, 28, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv4_2 (Conv2D)             (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 28, 28, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv4_3 (Conv2D)             (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv5_1 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv5_2 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv5_3 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "pool5 (MaxPooling2D)         (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 7, 7, 512)         2048      \n",
            "_________________________________________________________________\n",
            "fc1 (Conv2D)                 (None, 1, 1, 4096)        102764544 \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "fc3 (Dense)                  (None, 2)                 8194      \n",
            "=================================================================\n",
            "Total params: 134,318,414\n",
            "Trainable params: 134,293,576\n",
            "Non-trainable params: 24,838\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu0cWD8edlRK",
        "colab_type": "code",
        "outputId": "f0dc3cc0-dcd2-4cb0-a5dc-1676b1239c4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "# model train\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 19749 samples\n",
            "Epoch 1/5\n",
            "19749/19749 [==============================] - 441s 22ms/sample - loss: 32.0828 - accuracy: 0.8815\n",
            "Epoch 2/5\n",
            "19749/19749 [==============================] - 435s 22ms/sample - loss: 2.5786 - accuracy: 0.8959\n",
            "Epoch 3/5\n",
            "19749/19749 [==============================] - 434s 22ms/sample - loss: 2.0732 - accuracy: 0.8659\n",
            "Epoch 4/5\n",
            "19749/19749 [==============================] - 434s 22ms/sample - loss: 2.7691 - accuracy: 0.8441\n",
            "Epoch 5/5\n",
            "19749/19749 [==============================] - 435s 22ms/sample - loss: 2.3017 - accuracy: 0.8699\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe711809588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3OPMQmBwZr4",
        "colab_type": "text"
      },
      "source": [
        "Model Evaluation\n",
        "\n",
        "Test dataset is set of only face images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrh3Jr_UMLSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_images = np.load('/content/drive/My Drive/Colab Notebooks/test_images_save.npy')\n",
        "test_labels = np.load('/content/drive/My Drive/Colab Notebooks/test_labels_save.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rHDzIm5K87j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = tf.convert_to_tensor(test_images, dtype=tf.float32)\n",
        "y_test = tf.convert_to_tensor(test_labels, dtype=tf.float32)\n",
        "\n",
        "x_test = x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmVBrCpuedAb",
        "colab_type": "code",
        "outputId": "5c1489b2-4504-4dae-821c-829e4ea81328",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# model evaluate\n",
        "model.evaluate(x_test, y_test, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100/1 - 4s - loss: 4.8659 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.863677768707276, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fJv9hU_sme9",
        "colab_type": "code",
        "outputId": "f136dd59-46e3-4587-97cf-c2a9202899fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "model.predict(x_test[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.4394248 , 0.5605751 ],\n",
              "       [0.44259417, 0.5574058 ],\n",
              "       [0.44719857, 0.55280143],\n",
              "       [0.4590391 , 0.5409609 ],\n",
              "       [0.44431838, 0.55568165],\n",
              "       [0.44096082, 0.5590392 ],\n",
              "       [0.4571382 , 0.54286176],\n",
              "       [0.46420243, 0.5357976 ],\n",
              "       [0.44153726, 0.5584627 ],\n",
              "       [0.44923243, 0.5507676 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf7fQFUzJRSZ",
        "colab_type": "text"
      },
      "source": [
        "Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvbqwMxcJSnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('/content/drive/My Drive/Colab Notebooks/vgg16_face_checkpoint')\n",
        "model.save('/content/drive/My Drive/Colab Notebooks/vgg16_face.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}